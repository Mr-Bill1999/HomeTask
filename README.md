# Логистическая регрессия для задачи бинарной классификации

## Почему выбрана логистическая регрессия?

1. **Подходит для бинарной классификации**: огистическая регрессия хорошо подходит для задач, где необходимо предсказать принадлежность объекта к одному из двух классов. Она использует сигмоидную функцию для предсказания вероятности класса.


## Как работает градиентный спуск?

Градиентный спуск — это метод оптимизации, направленный на минимизацию функции потерь.

### Этапы работы

1. **Градиент**:
   - Рассчитывается вектор производных функции потерь по параметрам модели (весам и смещению).  
   - Этот вектор указывает направление наибольшего роста функции.  

   Формулы для логистической регрессии:
   - Для весов:
     \[
     \frac{\partial J}{\partial w_j} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \sigma(w \cdot x_i + b)) x_{ij}
     \]
   - Для смещения:
     \[
     \frac{\partial J}{\partial b} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \sigma(w \cdot x_i + b))
     \]

2. **Обновление параметров**:
   - Параметры обновляются в направлении, противоположном градиенту:
     \[
     w_j = w_j - \eta \frac{\partial J}{\partial w_j}, \quad b = b - \eta \frac{\partial J}{\partial b}
     \]
   - Здесь \(\eta\) — скорость обучения.

3. **Цель**:
   - Найти такие параметры (\(w\) и \(b\)), при которых функция потерь минимальна.

### Почему выбран градиентный спуск?

- **Эффективность**: Подходит для моделей с большим количеством параметров.
- **Адаптивность**: Может работать с большими объемами данных и сходится к оптимальному решению.

---

## Методы работы с дисбалансом классов

### Undersampling

1. **Принцип работы**:
   - Уменьшение числа объектов мажоритарного класса (большего по количеству) до уровня миноритарного (меньшего по количеству).
   - Выполняется случайным выбором подмножества данных из мажоритарного класса.

2. **Преимущества**:
   - Простота реализации.
   - Снижает дисбаланс классов, уменьшая смещение модели в сторону мажоритарного класса.

3. **Недостатки**:
   - Возможна потеря информации из-за исключения данных мажоритарного класса.
   - Может быть недостаточно эффективным при сильном дисбалансе классов.

### SMOTE (Synthetic Minority Oversampling Technique)

1. **Принцип работы**:
   - Генерация синтетических объектов для миноритарного класса путем интерполяции между существующими объектами.
   - Например, новый объект вычисляется как точка на отрезке между объектом миноритарного класса и его ближайшим соседом:
     \[
     x_{\text{new}} = x_i + \lambda (x_{\text{neighbor}} - x_i), \quad \lambda \in [0, 1]
     \]

2. **Преимущества**:
   - Сохраняет всю информацию из мажоритарного класса.
   - Увеличивает размер миноритарного класса без простого дублирования данных.

3. **Недостатки**:
   - Возможность внесения шума, если синтетические объекты создаются из пограничных точек.
   - Более сложная реализация и увеличение вычислительной нагрузки.


